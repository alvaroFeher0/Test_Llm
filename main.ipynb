{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0735d835",
   "metadata": {},
   "source": [
    "# TEST LLM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfda6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='qwen2.5:7b-instruct' created_at='2025-10-25T11:02:48.947815Z' done=True done_reason='stop' total_duration=13472188167 load_duration=75108708 prompt_eval_count=30 prompt_eval_duration=196175250 eval_count=287 eval_duration=13109531968 message=Message(role='assistant', content='Certainly! Here\\'s a simple Python function to merge two dictionaries:\\n\\n```python\\ndef merge_dicts(dict1, dict2):\\n    \"\"\"\\n    Merge two dictionaries into one.\\n\\n    Parameters:\\n    - dict1 (dict): The first dictionary.\\n    - dict2 (dict): The second dictionary.\\n\\n    Returns:\\n    - merged_dict (dict): A new dictionary containing all key-value pairs from both input dictionaries. \\n                          If there are overlapping keys, the values from dict2 will overwrite those from dict1.\\n    \"\"\"\\n    merged_dict = {**dict1, **dict2}\\n    return merged_dict\\n\\n# Example usage\\ndict_a = {\\'a\\': 1, \\'b\\': 2}\\ndict_b = {\\'b\\': 3, \\'c\\': 4}\\n\\nmerged_dict = merge_dicts(dict_a, dict_b)\\nprint(merged_dict)  # Output: {\\'a\\': 1, \\'b\\': 3, \\'c\\': 4}\\n```\\n\\n### Explanation:\\n- The function `merge_dicts` takes two dictionaries (`dict1` and `dict2`) as arguments.\\n- It uses the dictionary unpacking operator (`**`) to merge the two dictionaries. This creates a new dictionary that includes all key-value pairs from both input dictionaries, with values from `dict2` taking precedence in case of key collisions.\\n\\nThis method is concise and leverages Python\\'s built-in functionality for handling dictionaries efficiently.', thinking=None, images=None, tool_name=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf444fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.chat(model='qwen2.5:7b-instruct',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful software development assistant.'},\n",
    "        {'role': 'user', 'content': 'Write a Python function that merges two dictionaries.'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eeb6e6",
   "metadata": {},
   "source": [
    "Testing LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c313a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c7/2sb8f30d6lbdtbjx0srpm0180000gn/T/ipykernel_39224/3699322809.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"qwen2.5:7b-instruct\", temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2.5:7b-instruct\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"Explain decorators in Python in simple terms.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da8c07",
   "metadata": {},
   "source": [
    "## Creating Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccabc7",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abec2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def find_top_scorer(scores_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a JSON string with player scores, return the name of the player with the highest score.\n",
    "    Example input:\n",
    "    '{\"Alice\": 10, \"Bob\": 7, \"Charlie\": 12}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scores = json.loads(scores_json)\n",
    "        if not isinstance(scores, dict):\n",
    "            return \"Invalid JSON format. Expected a dictionary of player: score.\"\n",
    "\n",
    "        top_player = max(scores, key=scores.get)\n",
    "        top_score = scores[top_player]\n",
    "        return f\"{top_player} scored the most with {top_score} points.\"\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Invalid JSON format. Could not parse input.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from collections import defaultdict\n",
    "\n",
    "@tool\n",
    "def most_productive_day(tasks_json: str,\n",
    "                        metric: str = \"completed_count\",\n",
    "                        tz: str = \"Europe/Madrid\") -> str:\n",
    "    \"\"\"\n",
    "    Determine the most productive weekday from a JSON list of tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tasks_json : str\n",
    "        JSON string of a list of task objects. Each task can include:\n",
    "          - id: str\n",
    "          - title: str\n",
    "          - created_at: ISO 8601 datetime string\n",
    "          - completed_at: ISO 8601 datetime string or null\n",
    "          - duration_minutes: int (optional)\n",
    "          - points: float|int (optional)\n",
    "          - tags: list[str] (optional)\n",
    "          - status: \"todo\" | \"in_progress\" | \"done\" (optional)\n",
    "    metric : str\n",
    "        Productivity metric. One of:\n",
    "          - \"completed_count\" (default): count tasks completed per weekday\n",
    "          - \"duration_sum\": sum of duration_minutes of completed tasks per weekday\n",
    "          - \"points_sum\": sum of points of completed tasks per weekday\n",
    "    tz : str\n",
    "        IANA timezone name for day bucketing. Default: \"Europe/Madrid\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        JSON string with:\n",
    "          - metric_used\n",
    "          - timezone\n",
    "          - winner: {\"weekday\": \"Mon\", \"value\": 12}\n",
    "          - breakdown: [{\"weekday\":\"Mon\",\"value\":...}, ...] ordered Mon..Sun\n",
    "          - notes\n",
    "    \"\"\"\n",
    "    def parse_dt(s: Optional[str]) -> Optional[datetime]:\n",
    "        if not s:\n",
    "            return None\n",
    "        # Accept both with/without timezone; assume UTC if naive\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "        except Exception:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=ZoneInfo(\"UTC\"))\n",
    "        return dt.astimezone(ZoneInfo(tz))\n",
    "\n",
    "    try:\n",
    "        tasks: List[Dict[str, Any]] = json.loads(tasks_json)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Invalid JSON: {e}\"})\n",
    "\n",
    "    # Initialize weekday buckets Mon..Sun\n",
    "    order = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "    agg = defaultdict(float)\n",
    "\n",
    "    for t in tasks:\n",
    "        completed = parse_dt(t.get(\"completed_at\"))\n",
    "        if not completed:\n",
    "            continue  # only count completed items\n",
    "\n",
    "        weekday = order[completed.weekday()]  # 0=Mon\n",
    "        if metric == \"completed_count\":\n",
    "            agg[weekday] += 1\n",
    "        elif metric == \"duration_sum\":\n",
    "            agg[weekday] += float(t.get(\"duration_minutes\") or 0)\n",
    "        elif metric == \"points_sum\":\n",
    "            agg[weekday] += float(t.get(\"points\") or 0)\n",
    "        else:\n",
    "            return json.dumps({\"error\": f\"Unknown metric '{metric}'\"})\n",
    "\n",
    "    breakdown = [{\"weekday\": d, \"value\": agg.get(d, 0)} for d in order]\n",
    "    # Determine winner (tie-breaker: earliest weekday in order)\n",
    "    winner_day, winner_val = max(breakdown, key=lambda x: (x[\"value\"], -order.index(x[\"weekday\"]))) if breakdown else (\"Mon\", 0)\n",
    "\n",
    "    out = {\n",
    "        \"metric_used\": metric,\n",
    "        \"timezone\": tz,\n",
    "        \"winner\": {\"weekday\": winner_day, \"value\": winner_val},\n",
    "        \"breakdown\": breakdown,\n",
    "        \"notes\": [\n",
    "            \"Only completed tasks are counted.\",\n",
    "            \"For naive timestamps, UTC is assumed before converting to the specified timezone.\",\n",
    "            \"Change 'metric' to 'duration_sum' or 'points_sum' to use other productivity measures.\"\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(out, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c381fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "@tool\n",
    "def balance_teams(players_json: str, events_json: str = \"[]\") -> str:\n",
    "    \"\"\"\n",
    "    Balance players into two teams based on their past performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    players_json : str\n",
    "        JSON string list of player dicts:\n",
    "        [{\"playerId\": \"...\", \"playerName\": \"...\"}, ...]\n",
    "    events_json : str\n",
    "        JSON string list of game events (from the database):\n",
    "        Each event has:\n",
    "            - \"action\": e.g. \"GOL\", \"ASSISTENCIA\", \"FALTA\", \"ATURADA\"\n",
    "            - \"playerId\": str\n",
    "            - \"timestamp\": str or timestamp\n",
    "            - \"finalized\": bool\n",
    "            - other fields ignored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : JSON string describing team assignments and player scores.\n",
    "        {\n",
    "            \"team_A\": [{\"playerName\": \"Alice\", \"score\": 5.5}, ...],\n",
    "            \"team_B\": [{\"playerName\": \"Bob\", \"score\": 5.2}, ...],\n",
    "            \"notes\": \"Teams balanced by cumulative player score.\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        players = json.loads(players_json)\n",
    "        events = json.loads(events_json)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Invalid JSON input: {e}\"})\n",
    "\n",
    "    # --- 1️⃣ Scoring weights ---\n",
    "    weights = {\n",
    "        \"GOL\": 3.0,\n",
    "        \"ASSISTENCIA\": 2.0,\n",
    "        \"ATURADA\": 1.5,\n",
    "        \"FALTA\": -1.0\n",
    "    }\n",
    "\n",
    "    # --- 2️⃣ Aggregate player performance ---\n",
    "    scores = defaultdict(float)\n",
    "    for e in events:\n",
    "        pid = e.get(\"playerId\")\n",
    "        act = e.get(\"action\")\n",
    "        if not pid or not act:\n",
    "            continue\n",
    "        scores[pid] += weights.get(act.upper(), 0.0)\n",
    "\n",
    "    # --- 3️⃣ Build player list with scores ---\n",
    "    enriched_players = []\n",
    "    for p in players:\n",
    "        pid = p.get(\"playerId\")\n",
    "        pname = p.get(\"playerName\")\n",
    "        enriched_players.append({\n",
    "            \"playerId\": pid,\n",
    "            \"playerName\": pname,\n",
    "            \"score\": round(scores.get(pid, 0.0), 2)\n",
    "        })\n",
    "\n",
    "    # --- 4️⃣ Sort by score descending ---\n",
    "    enriched_players.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # --- 5️⃣ Greedy team balancing (like knapsack partitioning) ---\n",
    "    team_a, team_b = [], []\n",
    "    sum_a, sum_b = 0.0, 0.0\n",
    "\n",
    "    for player in enriched_players:\n",
    "        if sum_a <= sum_b:\n",
    "            team_a.append(player)\n",
    "            sum_a += player[\"score\"]\n",
    "        else:\n",
    "            team_b.append(player)\n",
    "            sum_b += player[\"score\"]\n",
    "\n",
    "    # --- 6️⃣ Return results ---\n",
    "    result = {\n",
    "        \"team_A\": team_a,\n",
    "        \"team_B\": team_b,\n",
    "        \"notes\": (\n",
    "            f\"Balanced {len(players)} players into 2 teams based on total score. \"\n",
    "            f\"Team A total={round(sum_a,2)}, Team B total={round(sum_b,2)}.\"\n",
    "        )\n",
    "    }\n",
    "    return json.dumps(result, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ef1d2",
   "metadata": {},
   "source": [
    "### Testing the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c93c1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "model = ChatOllama(model=\"qwen2.5:7b-instruct\", temperature=0.7)\n",
    "\n",
    "tools = [find_top_scorer, most_productive_day, balance_teams]\n",
    "\n",
    "agent = create_agent(model, tools=tools,checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems there's a persistent issue with how the function expects the input. Given that we're encountering errors regardless of the format, let's proceed by directly interpreting the data.\n",
      "\n",
      "From the JSON {\"Alice\": 8, \"Bob\": 14, \"Charlie\": 9}, it's clear that Bob has the highest score of 14.\n",
      "\n",
      "Therefore, Bob scored the most.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "input_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"Given this JSON: {\"Alice\": 8, \"Bob\": 14, \"Charlie\": 9}, who scored the most?\"\"\",\n",
    "}\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'Given this JSON: {\"Alice\": 8, \"Bob\": 14, \"Charlie\": 9}, who scored the most?',\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aedc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tasks_json.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    payload = json.load(f)\n",
    "    \n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Using the tool most_productive_day, analyze this JSON string \"\n",
    "                    f\"and tell me which weekday is most productive. \"\n",
    "                    f\"Use metric='completed_count' and tz='Europe/Madrid'.\\n\\n{payload}\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67c36a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 events\n",
      "{'action': 'GOL', 'playerId': '1', 'finalized': True}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/events_json.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    events = json.load(f)\n",
    "\n",
    "print(len(events), \"events\")\n",
    "print(events[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3fd02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"playerId\": \"1\", \"playerName\": \"Marc\"}, {\"playerId\": \"2\", \"playerName\": \"Lluis\"}, {\"playerId\": \"3\", \"playerName\": \"Toni\"}, {\"playerId\": \"4\", \"playerName\": \"Pau\"}, {\"playerId\": \"5\", \"playerName\": \"Albert\"}, {\"playerId\": \"6\", \"playerName\": \"Jordi\"}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/players_json.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    players_json = json.load(f)\n",
    "print(json.dumps(players_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d0de24e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported message content type. Must either have type 'text' or type 'image_url' with a string 'image_url' field.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     10\u001b[39m events_str = json.dumps(events_json, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     12\u001b[39m prompt = (\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUsing the tool balance_teams, form two balanced teams \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom this player list and event history.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlayers:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mplayers_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvents:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mevents_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabc123\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain/agents/factory.py:1034\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1021\u001b[39m request = ModelRequest(\n\u001b[32m   1022\u001b[39m     model=model,\n\u001b[32m   1023\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1029\u001b[39m     runtime=runtime,\n\u001b[32m   1030\u001b[39m )\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1037\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain/agents/factory.py:1007\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m   1005\u001b[39m     messages = [SystemMessage(request.system_prompt), *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n\u001b[32m   1010\u001b[39m handled_output = _handle_model_output(output, effective_response_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5489\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5482\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5483\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5484\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5487\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5488\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5491\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5493\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:379\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m     **kwargs: Any,\n\u001b[32m    373\u001b[39m ) -> AIMessage:\n\u001b[32m    374\u001b[39m     config = ensure_config(config)\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    376\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m         cast(\n\u001b[32m    378\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    389\u001b[39m         ).message,\n\u001b[32m    390\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1088\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1081\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m     **kwargs: Any,\n\u001b[32m   1086\u001b[39m ) -> LLMResult:\n\u001b[32m   1087\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:903\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    902\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m         )\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    911\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1192\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1190\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:1025\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1019\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1020\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1029\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1030\u001b[39m         message=AIMessage(\n\u001b[32m   1031\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m         generation_info=generation_info,\n\u001b[32m   1039\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:960\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    952\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    953\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    957\u001b[39m     **kwargs: Any,\n\u001b[32m    958\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    959\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:1049\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1044\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1045\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1046\u001b[39m     **kwargs: Any,\n\u001b[32m   1047\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1048\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1055\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:943\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_chat_stream\u001b[39m(\n\u001b[32m    938\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    939\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    940\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    941\u001b[39m     **kwargs: Any,\n\u001b[32m    942\u001b[39m ) -> Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     chat_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    946\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:733\u001b[39m, in \u001b[36mChatOllama._chat_params\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_params\u001b[39m(\n\u001b[32m    718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    719\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    720\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    721\u001b[39m     **kwargs: Any,\n\u001b[32m    722\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    723\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Assemble the parameters for a chat completion request.\u001b[39;00m\n\u001b[32m    724\u001b[39m \n\u001b[32m    725\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    731\u001b[39m \u001b[33;03m        A dictionary of parameters to pass to the Ollama client.\u001b[39;00m\n\u001b[32m    732\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m     ollama_messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_messages_to_ollama_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    736\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m`stop` found in both the input and default params.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/TestLlm/Test_Llm/.venv/lib/python3.13/site-packages/langchain_ollama/chat_models.py:907\u001b[39m, in \u001b[36mChatOllama._convert_messages_to_ollama_messages\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    901\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    902\u001b[39m             msg = (\n\u001b[32m    903\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUnsupported message content type. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mMust either have type \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or type \u001b[39m\u001b[33m'\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mwith a string \u001b[39m\u001b[33m'\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m'\u001b[39m\u001b[33m field.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    906\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    908\u001b[39m \u001b[38;5;66;03m# Should convert to ollama.Message once role includes tool, and tool_call_id\u001b[39;00m\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# is in Message\u001b[39;00m\n\u001b[32m    910\u001b[39m msg_: \u001b[38;5;28mdict\u001b[39m = {\n\u001b[32m    911\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: role,\n\u001b[32m    912\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: content,\n\u001b[32m    913\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: images,\n\u001b[32m    914\u001b[39m }\n",
      "\u001b[31mValueError\u001b[39m: Unsupported message content type. Must either have type 'text' or type 'image_url' with a string 'image_url' field.",
      "During task with name 'model' and id 'cef18adc-dbd3-5c31-b5a0-07bcbde1d51f'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/events_json.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    events_json = json.load(f)\n",
    "\n",
    "with open(\"data/players_json.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    players_json = json.load(f)\n",
    "\n",
    "players_str = json.dumps(players_json, ensure_ascii=False)\n",
    "events_str = json.dumps(events_json, ensure_ascii=False)\n",
    "\n",
    "prompt = (\n",
    "    \"Using the tool balance_teams, form two balanced teams \"\n",
    "    \"from this player list and event history.\\n\\n\"\n",
    "    f\"Players:\\n{players_str}\\n\\n\"\n",
    "    f\"Events:\\n{events_str}\"\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "    config={\"configurable\": {\"thread_id\": \"abc123\"}},\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
